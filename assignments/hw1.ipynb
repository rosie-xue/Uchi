{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHdhDx3SfbNa"
      },
      "source": [
        "# **Homework 1: Bayesian linear regression**\n",
        "STAT 348, UChicago, Spring 2025\n",
        "\n",
        "----------------\n",
        "**Your name here:** Xueran Tao\n",
        "\n",
        "**Hours spent:**\n",
        "\n",
        "(Please let us know how many hours in total you spent on this assignment so we can calibrate for future assignments. Your feedback is always welcome!)\n",
        "\n",
        "----------------\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/aschein/stat_348_2025/blob/main/assignments/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This homework focuses on themes in the first three lectures and will also get you familiar with Python and PyTorch which we will use for the rest of the course.\n",
        "\n",
        "For reference, this homework is a close adaption of [HW1 for Scott Linderman's STATS 305C](https://github.com/slinderman/stats305c/blob/spring2023/assignments/hw1/hw1.ipynb), for which the [slides for lecture 1](https://github.com/slinderman/stats305c/blob/spring2023/slides/lecture01-bayes_normal.pdf) may be a useful reference.\n",
        "\n",
        "Assignment is due **Sunday April 6, 11:59pm** on GradeScope.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-_CGkTvS9dH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributions import Normal, Gamma, \\\n",
        "    TransformedDistribution, MultivariateNormal\n",
        "from torch.distributions.transforms import PowerTransform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6k2BC4ZeFpS"
      },
      "source": [
        "## Bayesian Linear Regression\n",
        "\n",
        "Let $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ denote a dataset with covariates $\\mathbf{x}_i \\in \\mathbb{R}^p$ and scalar outcomes $y_i \\in \\mathbb{R}$. Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ denote the design matrix where each row is a vector of covariates and $\\mathbf{y} \\in \\mathbb{R}^n$ denote the vector of outcomes.\n",
        "\n",
        "We will model the outcomes as conditionally independent Gaussian random variables given the covariates and the parameters,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\mathbf{y} \\mid \\boldsymbol{\\beta}, \\sigma^2, \\mathbf{X})\n",
        "&= \\prod_{i=1}^N \\mathcal{N}(y_i \\mid \\mathbf{x}_i^\\top \\boldsymbol{\\beta},\\, \\sigma^2),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are the _regression coefficients_ and $\\sigma^2 \\in \\mathbb{R}_+$ is the _conditional variance_.\n",
        "\n",
        "As discussed in [lecture 2](https://github.com/aschein/stat_348_2025/blob/main/lecture_materials/ipad_notes/lecture_2.pdf), in _Bayesian_ linear regression we place a priors over the parameters. In lecture, we placed a simple multivariate Gaussian prior over the coefficients $\\boldsymbol{\\beta}$ and treated the variance $\\sigma^2$ as a fixed and known hyperparameter. In this homework, we will place a _joint prior over both_ parameters. We first place a _scaled inverse chi-squared_ prior over $\\sigma^2$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(\\sigma^2 \\mid v_0, \\tau_0^2) &= \\chi^{-2}(\\sigma^2;\\, v_0, \\tau_0^2) \\\\\n",
        "&= \\frac{(\\tfrac{\\nu_0 \\tau_0^2}{2})^{\\tfrac{\\nu_0}{2}}}{\\Gamma(\\tfrac{\\nu_0}{2})} (\\sigma^2)^{-\\tfrac{\\nu_0}{2}-1}\\exp(-\\tfrac{\\nu_0\\tau_0^2}{2\\sigma^2})\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\nu_0 \\in \\mathbb{R}_+$ is the _prior degrees of freedom_ and $\\tau_0^2 \\in \\mathbb{R}_+$ is the _prior mean_ of $\\sigma_2$. We then place a Gaussian prior over $\\boldsymbol{\\beta}$:\n",
        "$$P(\\boldsymbol{\\beta} \\mid \\sigma^2, \\mathbf{m}_0,\\, L_0) = \\mathcal{N}(\\boldsymbol{\\beta};\\, \\textbf{m}_0, \\sigma^2 L_0^{-1})$$\n",
        "where $\\mathbf{m}_0 \\in \\mathbb{R}^p$ is the _prior mean_ of the coefficients, and $L_0$ is a positive definite $p \\times p$ _precision matrix_. We collect all *hyperparameters* into the vector $\\boldsymbol{\\eta}_0 = (\\nu_0, \\tau_0^2, \\mathbf{m}_0, L_0)$.\n",
        "\n",
        "Notice that the prior over $\\boldsymbol{\\beta}$ _depends on_ $\\sigma^2$. We can equivalently express the joint prior over both parameters as the _normal inverse chi-squared distribution (NIX)_:\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\boldsymbol{\\eta}_0) &= \\textrm{NIX}(\\boldsymbol{\\beta},\\,\\sigma^2;\\,  \\textbf{m}_0, L_0, v_0, \\tau_0^2) \\\\\n",
        "&= \\chi^{-2}(\\sigma^2;\\, v_0, \\tau_0^2) \\,\\mathcal{N}(\\boldsymbol{\\beta};\\, \\textbf{m}_0, \\sigma^2 L_0^{-1})\n",
        "\\end{align*}\n",
        "$$\n",
        "The **normal inverse chi-squared (NIX) distribution is a conjugate prior for the likelihood** in equation 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gFPVwd4Q3KR"
      },
      "source": [
        "\n",
        "## PyTorch\n",
        "\n",
        "You will use PyTorch to complete the coding portions of this assignment. If you are unfamiliar with PyTorch, [this](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) webpage provides an introductory tutorial to PyTorch tensors. Another good resource is [homework 0 of STAT 305C](https://github.com/slinderman/stats305c/blob/spring2023/assignments/hw0/hw0.ipynb), which you could work through for practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCoqcOs8RnS1"
      },
      "source": [
        "## Problem 1: Derive the Posterior [Math]\n",
        "\n",
        "Derive the posterior distribution $p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathbf{y}, X, \\boldsymbol{\\eta}_0)$ where $\\boldsymbol{\\eta}_0 = (\\nu_0, \\sigma_0^2, \\mathbf{m}_0, L_0)$. Since the NIX distribution is the conjugate prior, the posterior should be of the same form as the prior (i.e., another NIX distribution):\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathbf{y}, X, \\boldsymbol{\\eta}_0)\n",
        "&= \\textrm{NIX}(\\boldsymbol{\\beta},\\,\\sigma^2;\\,  \\mathbf{m}_n, L_n, v_n, \\tau_n^2) \\\\\n",
        "&= \\chi^{-2}(\\sigma^2 \\mid \\nu_n, \\tau_n^2) \\mathcal{N}(\\boldsymbol{\\beta} \\mid \\mathbf{m}_n, \\sigma^2 L_n^{-1})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "for some _posterior parameters_ $\\nu_n$, $\\tau_n^2$, $\\mathbf{m}_n$, and $L_n$. Your job is to provide the exact form of these parameters.\n",
        "\n",
        "**Hint 1:** Remember that the \"standard procedure\" for deriving the posterior distribution is to write down the joint distribution (on both parameters and data), and then only collect the terms involving the parameters to obtain the \"kernel\" of the posterior. But, in this setting, you have to be very careful to keep both $\\boldsymbol{\\beta}$ and $\\sigma^2$, because we are asking for the _joint posterior_.\n",
        "\n",
        "**Hint 2:** When working with quadratic forms, a useful operation is to complete the square; for any $\\mathbf{a} \\in \\mathbb{R}^n$, $B \\in \\mathbb{R}^{n \\times n}$, and $\\mathbf{c} \\in \\mathbb{R}^n$:\n",
        "$$\\mathbf{a}^\\top B \\mathbf{a} - 2\\mathbf{a}^\\top B \\mathbf{c} = (\\mathbf{a} - \\mathbf{c})^\\top B (\\mathbf{a} - \\mathbf{c}) - \\mathbf{c}^\\top B \\mathbf{c}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDrwyQmdR44o"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYbTI5tcocJM"
      },
      "source": [
        "## Problem 2: The Posterior Mean [Math]\n",
        "a. What does the posterior mean $\\mathbb{E}[\\boldsymbol{\\beta} \\mid \\mathbf{y}, X, \\boldsymbol{\\eta}_0]$ equal in the uninformative limit where $L_0 \\to 0$ and $\\nu_0 \\to 0$?\n",
        "\n",
        "b. What does the posterior mean $\\mathbb{E}[\\sigma^2 \\mid \\mathbf{y}, X, \\boldsymbol{\\eta}_0]$ equal in the uninformative limit where $L_0 \\to 0$ and $\\nu_0 \\to 0$? Write your answer in terms of the _hat matrix_ $\\mathbf{H} = X (X^\\top X)^{-1} X^\\top$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJDQjtrhoubw"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LZXHq9yW4o"
      },
      "source": [
        "## Synthetic Data\n",
        "\n",
        "We'll do some simple analysis of a synthetic dataset with $n =20$ data points. Each data point has covariates $\\mathbf{x}_i = (1, x_i) \\in \\mathbb{R}^2$ and scalar outcomes $y_i \\in \\mathbb{R}$. It looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rZ3PvD2yuki",
        "outputId": "de28adfc-bbdd-4b02-e3d6-9d10823d2f35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, '$y$')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download the data (uncomment the line below)\n",
        "# !wget https://github.com/aschein/stat_348_2025/raw/main/assignments/hw1.pt\n",
        "\n",
        "# Load the data.\n",
        "# X = [[1, x_1]\n",
        "#      [1, x_2]\n",
        "#         ...\n",
        "#      [1, x_N]]\n",
        "#\n",
        "# y = [y_1, ..., y_N]\n",
        "X, y = torch.load(\"hw1.pt\")\n",
        "\n",
        "plt.plot(X[:, 1], y, 'ko')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiagNPRRzXKD"
      },
      "source": [
        "Here, the outcomes were simulated from a linear regression with Gaussian noise according to some true parameters (not given). You will compute and visualize the posterior distribution over the weights and variance given the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI4tX9AGquoL"
      },
      "source": [
        "## Problem 3: Compute the posterior [Code]\n",
        "\n",
        "Write a function to compute the posterior parameters given data and hyperparameters.\n",
        "\n",
        "*Hints*: You may find the following commands in PyTorch useful:\n",
        "- If ```a``` is a tensor, ```a.shape``` is a tuple containing the shape of ```a```.\n",
        "- If ```a``` is a tensor, ```a.T``` returns the transpose of ```a```.\n",
        "- ```torch.linalg.solve```\n",
        "- ```*``` denotes element-wise multiplication while ```@``` denotes standard matrix-matrix or matrix-vector multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty4VpJVV67Kn"
      },
      "outputs": [],
      "source": [
        "def compute_posterior(X, y, nu_0, tau_0, m_0, L_0):\n",
        "    \"\"\"\n",
        "    Compute the posterior parameters nu_n, tau_n, m_n, and L_n\n",
        "    given covariates X, outcomes y, and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        X:          (n, p) tensor of covariates\n",
        "        y:          (n,) tensor of outcomes\n",
        "        nu_0:       prior degrees of freedom\n",
        "        tau_0:  prior mean of the variance parameter\n",
        "        m_0:       prior mean of the weights\n",
        "        L_0:   prior precision of the weights\n",
        "\n",
        "    Returns:\n",
        "        nu_n:       posterior degrees of freedom\n",
        "        tau_n:  posterior scale of the variance parameter\n",
        "        m_n:       posterior mean of the weights\n",
        "        L_n:   posterior precision of the weights\n",
        "    \"\"\"\n",
        "    ##\n",
        "    # YOUR CODE HERE\n",
        "    #\n",
        "    ##\n",
        "\n",
        "    return nu_n, tau_n, m_n, L_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXnTwuLHyAfR"
      },
      "source": [
        "Please run the following code to print your answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bTSbgTNu5AI"
      },
      "outputs": [],
      "source": [
        "# Test:\n",
        "hyperparams = dict(\n",
        "    nu_0=torch.tensor(1.0),\n",
        "    tau_0=torch.tensor(1.0),\n",
        "    m_0=torch.zeros(2),\n",
        "    L_0=0.1 * torch.eye(2)\n",
        ")\n",
        "\n",
        "nu_n, tau_n, m_n, L_n = compute_posterior(X, y, **hyperparams)\n",
        "print(\"nu_n:       \\n\", nu_n)\n",
        "print(\"\")\n",
        "print(\"tau_n:  \\n\", tau_n)\n",
        "print(\"\")\n",
        "print(\"m_n:       \\n\", m_n)\n",
        "print(\"\")\n",
        "print(\"L_n:   \\n\", L_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAPSDTFz1KK"
      },
      "source": [
        "## Problem 4: Plot the posterior density of the variance [Code]\n",
        "\n",
        "Plot $p(\\sigma^2 \\mid X, \\mathbf{y}, \\boldsymbol{\\eta}_0)$ vs $\\sigma^2$ over the interval $[10^{-3}, 2]$, where $X$ and $\\mathbf{y}$ continue to be the synthetic data we downloaded and used in Problem 3.\n",
        "\n",
        "You may use the `ScaledInvChiSq` distribution object below, which we copied from the demo for Lecture 1.\n",
        "\n",
        "_Hint_: In Python, you can use `dir(object)` to list the attributes and functions that an object supports.\n",
        "\n",
        "_Hint_: To learn more about PyTorch distributions, see the [docs](https://pytorch.org/docs/stable/distributions.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1ayM_2pz6z_"
      },
      "outputs": [],
      "source": [
        "class ScaledInvChiSq(TransformedDistribution):\n",
        "\n",
        "    def __init__(self, dof, scale):\n",
        "        \"\"\"\n",
        "        Implementation of the scaled inverse \\chi^2 distribution,\n",
        "\n",
        "        ..math:\n",
        "            \\chi^{-2}(\\nu_0, \\tau_0^2)\n",
        "\n",
        "        It is equivalent to an inverse gamma distribution, which we implement\n",
        "        as a transformation of a Gamma distribution. Thus, this class inherits\n",
        "        functions like `log_prob` from its parent.\n",
        "\n",
        "        Args:\n",
        "            dof:   degrees of freedom parameter\n",
        "            scale: scale of the $\\chi^{-2}$ distribution.\n",
        "        \"\"\"\n",
        "        base = Gamma(dof / 2, dof * scale / 2)\n",
        "        transforms = [PowerTransform(-1)]\n",
        "        TransformedDistribution.__init__(self, base, transforms)\n",
        "        self.dof = dof\n",
        "        self.scale = scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoO2dZZxBaVt"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epW71f9x03sn"
      },
      "source": [
        "## Problem 5: Plot posterior samples of the regression function. [Code]\n",
        "Draw 50 samples from the posterior marginal distribution over the weights $\\boldsymbol{\\beta} \\in \\mathbb{R}^2$. For each sample, compute the expected value of $y$ on a grid of points $x$ evenly spaced between $[-3, 3]$. Remember that our covariates were defined as $\\mathbf{x} = (1, x)$ so that for each sample of the weights you get a line for $\\mathbb{E}[y \\mid x, \\boldsymbol{\\beta}]$ as a function of $x$. Plot these 50 lines on top of each other to get a sense of the posterior uncertainty in the regression function. (You may want to plot each line with some transparency, like `alpha=0.1`.) Overlay the observed data points.\n",
        "\n",
        "*Hint*: You may find ```torch.inverse``` useful.\n",
        "\n",
        "*Hint*: Remember that in the generative model we have posited, the distribution of $\\boldsymbol{\\beta}$ depends on $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECdb34A401Kh"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# YOUR CODE HERE\n",
        "#\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcl9zCRs4YHt"
      },
      "source": [
        "## Problem 6: Posterior Predictive Distribution [Math]\n",
        "The subparts of this problem will walk you through deriving the posterior predictive distribution of the outcome at a new input $\\mathbf{x}_{n+1}$. That is, computing,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(y_{n+1} \\mid \\mathbf{x}_{n+1}, \\mathbf{y}, X, \\boldsymbol{\\eta}_0)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "integrating over the posterior distribution on the coefficients $\\boldsymbol{\\beta}$ and variance $\\sigma^2$.\n",
        "Remember that you found this posterior distribution in Problem 1, but for the purpose of this question it's enough to leave it in the form\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "p(\\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathbf{y}, X, \\boldsymbol{\\eta}_0)\n",
        "&= \\chi^{-2}(\\sigma^2 \\mid \\nu_n, \\tau_n^2) \\, \\mathcal{N}(\\boldsymbol{\\beta} \\mid \\mathbf{m}_n, \\sigma^2 L_n^{-1}),\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "i.e. you don't need to plug in the values for for some $\\nu_n$, $\\tau_n^2$, $\\mathbf{m}_n$, and $L_n$ that you found in Problem 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11OeZxrqOBz"
      },
      "source": [
        "### Problem 6a\n",
        "\n",
        "Using the product rule of probability, write out the joint distribution of the posterior over the parameters and the observation of the next data point $y_{n+1}$:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y_{n + 1}, \\boldsymbol{\\beta}, \\sigma^2 \\mid \\mathbf{x}_{n + 1}, \\mathbf{y}, X, \\boldsymbol{\\eta}_0).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "You can (and please do) replace any densities from a known family with the notation $\\text{symbol for the family}( \\text{variable name} \\mid \\text{parameters})$. (We follow this notation in how we write out the posterior above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxHvY2tk5I_Q"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-kOJ9TTss0S"
      },
      "source": [
        "### Problem 6b\n",
        "\n",
        "Now using the sum rule of probability, compute the posterior predictive distribution\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  p(y_{n + 1}, | \\mathbf{x}_{n + 1}, \\mathbf{y}, X, \\boldsymbol{\\eta}_0) = \\int  p(y_{n + 1}, \\boldsymbol{\\beta}, \\sigma^2 \\mid \\boldsymbol{x}_{n + 1}, \\mathbf{y}, X, \\boldsymbol{\\eta}_0) \\,d\\boldsymbol{\\beta} \\,d\\sigma^2.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "*Hint:* You can do this integral without taking any integrals! Think about conjugate families and how the Student's T distribution arises (e.g., see [these slides](https://github.com/slinderman/stats305c/blob/spring2023/slides/lecture01-bayes_normal.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVH5ZfdDtfjC"
      },
      "source": [
        "---\n",
        "\n",
        "_Your answer here._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM9vMLLP9J1n"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "\n",
        "**Formatting:** check that your code does not exceed 80 characters in line width. You can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit.\n",
        "\n",
        "Download your notebook in .ipynb format and use the following commands to convert it to PDF.  Then run the following command to convert to a PDF:\n",
        "```\n",
        "jupyter nbconvert --to pdf <yourlastname>_hw1.ipynb\n",
        "```\n",
        "(Note that for the above code to work, you need to rename your file `<yourlastname>_hw1.ipynb`)\n",
        "\n",
        "Possible causes for errors:\n",
        "  * the \"Open in colab\" button. Just delete the code that creates this button (go to the top cell and delete it)\n",
        "  * Latex errors: many latex errors aren't visible in the notebook. Try binary search: comment out half of the latex at a time, until you find the bugs\n",
        "\n",
        "Getting this HW into PDF form isn't meant to be a burden. One quick and easy approach is to open it as a Jupyter notebook, print, save to pdf. Just make sure your latex math answers aren't cut off so we can grade them.\n",
        "\n",
        "Please post on Ed or come to OH if there are any other problems submitting the HW.\n",
        "\n",
        "**Installing nbconvert:**\n",
        "\n",
        "If you're using Anaconda for package management,\n",
        "```\n",
        "conda install -c anaconda nbconvert\n",
        "```\n",
        "\n",
        "**Upload** your .pdf file to Gradescope. Please tag your questions!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "b6k2BC4ZeFpS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}